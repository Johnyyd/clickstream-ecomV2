# TÃ­nh nÄƒng phÃ¢n tÃ­ch má»›i - Analysis Features

## Tá»•ng quan

Há»‡ thá»‘ng Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t vá»›i cÃ¡c tÃ­nh nÄƒng phÃ¢n tÃ­ch linh hoáº¡t vÃ  máº¡nh máº½ hÆ¡n:

### 1. **PhÃ¢n tÃ­ch toÃ n bá»™ Database** (Máº·c Ä‘á»‹nh)
- PhÃ¢n tÃ­ch táº¥t cáº£ ngÆ°á»i dÃ¹ng vÃ  toÃ n bá»™ dá»¯ liá»‡u trong MongoDB
- Cung cáº¥p cÃ¡i nhÃ¬n tá»•ng quan vá» hÃ nh vi ngÆ°á»i dÃ¹ng trÃªn toÃ n há»‡ thá»‘ng
- GiÃºp hiá»ƒu xu hÆ°á»›ng chung vÃ  insights toÃ n cá»¥c

### 2. **PhÃ¢n tÃ­ch ngÆ°á»i dÃ¹ng cá»¥ thá»ƒ** (TÃ¹y chá»n)
- PhÃ¢n tÃ­ch hÃ nh vi cá»§a má»™t ngÆ°á»i dÃ¹ng duy nháº¥t
- Chá»‰ cáº§n nháº­p **username** Ä‘á»ƒ phÃ¢n tÃ­ch
- Há»¯u Ã­ch cho viá»‡c cÃ¡ nhÃ¢n hÃ³a vÃ  phÃ¢n tÃ­ch chi tiáº¿t

### 3. **Engine máº·c Ä‘á»‹nh: Apache Spark**
- **Spark** lÃ  engine phÃ¢n tÃ­ch máº·c Ä‘á»‹nh
- Xá»­ lÃ½ hiá»‡u quáº£ vá»›i dataset lá»›n
- Tá»± Ä‘á»™ng fallback vá» **Python** náº¿u Spark gáº·p lá»—i

---

## CÃ¡ch sá»­ dá»¥ng trÃªn Dashboard

### BÆ°á»›c 1: ÄÄƒng nháº­p
```
1. Truy cáº­p /dashboard
2. ÄÄƒng nháº­p vá»›i username vÃ  password
```

### BÆ°á»›c 2: Chá»n cháº¿ Ä‘á»™ phÃ¢n tÃ­ch

Sau khi Ä‘Äƒng nháº­p, báº¡n sáº½ tháº¥y 2 nÃºt:

#### ğŸ“Š **All Users (Database)** - Máº·c Ä‘á»‹nh
- Click nÃºt nÃ y Ä‘á»ƒ phÃ¢n tÃ­ch toÃ n bá»™ database
- PhÃ¢n tÃ­ch táº¥t cáº£ ngÆ°á»i dÃ¹ng vÃ  táº¥t cáº£ events
- PhÃ¹ há»£p cho bÃ¡o cÃ¡o tá»•ng quan

#### ğŸ‘¤ **Single User**
- Click nÃºt nÃ y Ä‘á»ƒ phÃ¢n tÃ­ch má»™t user cá»¥ thá»ƒ
- Nháº­p **username** vÃ o Ã´ input
- Click nÃºt "Run Spark Analysis" Ä‘á»ƒ cháº¡y

### BÆ°á»›c 3: Cháº¡y phÃ¢n tÃ­ch

```
1. Chá»n mode phÃ¢n tÃ­ch (All Users hoáº·c Single User)
2. Náº¿u chá»n Single User, nháº­p username
3. Click "Run Spark Analysis"
4. Äá»£i káº¿t quáº£ hiá»ƒn thá»‹
```

---

## API Endpoints

### POST `/api/analyze`

Cháº¡y phÃ¢n tÃ­ch vá»›i cÃ¡c tÃ¹y chá»n:

#### PhÃ¢n tÃ­ch toÃ n bá»™ database:
```json
{
  "params": {
    "analysis_target": "all",
    "use_spark": true
  }
}
```

#### PhÃ¢n tÃ­ch user cá»¥ thá»ƒ:
```json
{
  "params": {
    "analysis_target": "username:john_doe",
    "use_spark": true
  }
}
```

#### PhÃ¢n tÃ­ch user hiá»‡n táº¡i (máº·c Ä‘á»‹nh):
```json
{
  "params": {
    "use_spark": true
  }
}
```

---

## Cáº¥u hÃ¬nh

### Environment Variables

```bash
# Sá»­ dá»¥ng Spark lÃ m engine máº·c Ä‘á»‹nh (Ä‘Ã£ set máº·c Ä‘á»‹nh = true)
USE_SPARK=true

# Náº¿u muá»‘n dÃ¹ng Python lÃ m máº·c Ä‘á»‹nh
USE_SPARK=false
```

### Trong code

File `analysis.py`:
```python
# Default to TRUE - use Spark by default, fallback to Python on error
USE_SPARK = os.environ.get('USE_SPARK', 'true').lower() == 'true'
```

---

## Luá»“ng xá»­ lÃ½

```
User clicks "Run Analysis"
    â†“
Dashboard.js xÃ¡c Ä‘á»‹nh target:
  - 'all' â†’ ToÃ n bá»™ database
  - 'username:xxx' â†’ User cá»¥ thá»ƒ
  - None â†’ User hiá»‡n táº¡i
    â†“
API /api/analyze nháº­n request
    â†“
run_analysis(user_id, params)
  - user_id = None â†’ Analyze ALL
  - user_id = ObjectId â†’ Analyze specific user
    â†“
Try Spark Analysis
    â†“ (náº¿u lá»—i)
Fallback to Python Analysis
    â†“
Save results to MongoDB.analyses
    â†“
Return analysis ID to frontend
    â†“
Frontend hiá»ƒn thá»‹ káº¿t quáº£
```

---

## TÃ­nh nÄƒng bá»• sung

### Auto-Analyze
- Tá»± Ä‘á»™ng cháº¡y phÃ¢n tÃ­ch Ä‘á»‹nh ká»³ (má»—i 60 giÃ¢y)
- Sá»­ dá»¥ng analysis target hiá»‡n táº¡i
- Skip LLM Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™

### Real-time Metrics
- Cáº­p nháº­t metrics má»—i 10 giÃ¢y
- Hiá»ƒn thá»‹ biá»ƒu Ä‘á»“ realtime
- GiÃ¡m sÃ¡t events theo thá»i gian thá»±c

---

## Äá»“ng bá»™ dá»¯ liá»‡u

### Dashboard â†” MongoDB

Táº¥t cáº£ dá»¯ liá»‡u trÃªn dashboard luÃ´n Ä‘Æ°á»£c Ä‘á»“ng bá»™ vá»›i MongoDB:

1. **Events**: Má»i event Ä‘Æ°á»£c lÆ°u ngay vÃ o `events` collection
2. **Analyses**: Káº¿t quáº£ phÃ¢n tÃ­ch lÆ°u vÃ o `analyses` collection
3. **Sessions**: Session tracking trong `sessions` collection
4. **Real-time updates**: Polling API má»—i 10 giÃ¢y

### Äáº£m báº£o consistency

- Sá»­ dá»¥ng MongoDB ObjectId cho user_id
- Timestamp Ä‘Æ°á»£c chuáº©n hÃ³a vá» UTC
- Indexes Ä‘Æ°á»£c tá»‘i Æ°u cho queries

---

## Troubleshooting

### Spark khÃ´ng hoáº¡t Ä‘á»™ng?
- Kiá»ƒm tra Spark Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t Ä‘Ãºng
- Xem logs console Ä‘á»ƒ biáº¿t lá»—i cá»¥ thá»ƒ
- Há»‡ thá»‘ng sáº½ tá»± Ä‘á»™ng fallback vá» Python

### KhÃ´ng tÃ¬m tháº¥y username?
- Äáº£m báº£o username nháº­p chÃ­nh xÃ¡c
- Kiá»ƒm tra user tá»“n táº¡i trong database
- Error message sáº½ hiá»ƒn thá»‹ náº¿u user khÃ´ng tá»“n táº¡i

### Analysis cháº­m?
- Vá»›i dataset lá»›n, Spark analysis cÃ³ thá»ƒ máº¥t vÃ i phÃºt
- CÃ³ thá»ƒ giá»›i háº¡n sá»‘ events báº±ng tham sá»‘ `limit`
- Sá»­ dá»¥ng Auto-Analyze vá»›i skip_llm Ä‘á»ƒ tÄƒng tá»‘c

---

## VÃ­ dá»¥ sá»­ dá»¥ng

### Case 1: Marketing Team - PhÃ¢n tÃ­ch toÃ n bá»™
```
Má»¥c Ä‘Ã­ch: Hiá»ƒu behavior cá»§a táº¥t cáº£ users
CÃ¡ch lÃ m:
1. Login vÃ o dashboard
2. Giá»¯ mode "All Users (Database)" (máº·c Ä‘á»‹nh)
3. Click "Run Spark Analysis"
4. Xem insights tá»•ng quan vá» conversion, bounce rate, top pages
```

### Case 2: Customer Success - PhÃ¢n tÃ­ch VIP customer
```
Má»¥c Ä‘Ã­ch: PhÃ¢n tÃ­ch chi tiáº¿t behavior cá»§a customer quan trá»ng
CÃ¡ch lÃ m:
1. Login vÃ o dashboard
2. Click "Single User"
3. Nháº­p username: "vip_customer_123"
4. Click "Run Spark Analysis"
5. Xem insights chi tiáº¿t vÃ  personalized recommendations
```

### Case 3: Developer - Debug user issues
```
Má»¥c Ä‘Ã­ch: Debug váº¥n Ä‘á» cá»§a user cá»¥ thá»ƒ
CÃ¡ch lÃ m:
1. Login vÃ o dashboard
2. Click "Single User"
3. Nháº­p username cá»§a user gáº·p váº¥n Ä‘á»
4. Analyze Ä‘á»ƒ xem event timeline vÃ  behavior
5. Identify issues trong funnel
```

---

## Best Practices

1. **PhÃ¢n tÃ­ch Ä‘á»‹nh ká»³**: DÃ¹ng "All Users" hÃ ng ngÃ y Ä‘á»ƒ track metrics
2. **PhÃ¢n tÃ­ch sÃ¢u**: DÃ¹ng "Single User" khi cáº§n investigate
3. **Auto-Analyze**: Báº­t Ä‘á»ƒ giá»¯ dashboard luÃ´n cáº­p nháº­t
4. **Spark vs Python**: Tin tÆ°á»Ÿng vÃ o auto-fallback
5. **LLM Insights**: Táº¯t skip_llm khi cáº§n insights chi tiáº¿t

---

## Technical Details

### Database Schema

```javascript
// analyses collection
{
  _id: ObjectId,
  user_id: ObjectId | null,  // null = analyze all users
  created_at: ISODate,
  parameters: {
    analysis_target: "all" | "username:xxx" | undefined,
    use_spark: true,
    limit: null
  },
  status: "done" | "failed",
  spark_summary: { ... },
  detailed_metrics: { ... },
  insights: { ... }
}
```

### Performance

- **Spark**: Xá»­ lÃ½ 1M+ events trong < 5 phÃºt
- **Python**: PhÃ¹ há»£p vá»›i < 100K events
- **Auto-fallback**: KhÃ´ng downtime khi Spark fail

---

## Changelog

### Version 2.0 - Analysis Features Update

#### Added
- âœ… PhÃ¢n tÃ­ch toÃ n bá»™ database (analysis_target: "all")
- âœ… PhÃ¢n tÃ­ch theo username (analysis_target: "username:xxx")
- âœ… Spark lÃ m engine máº·c Ä‘á»‹nh (USE_SPARK=true)
- âœ… UI controls cho analysis mode selection
- âœ… Auto-analyze vá»›i target support
- âœ… Enhanced error handling vÃ  fallback

#### Changed
- ğŸ”„ USE_SPARK default: false â†’ true
- ğŸ”„ run_analysis() há»— trá»£ user_id = None
- ğŸ”„ API endpoint /analyze vá»›i analysis_target param

#### Fixed
- ğŸ› ObjectId handling khi user_id = None
- ğŸ› Dashboard sync vá»›i MongoDB
- ğŸ› Fallback mechanism tá»« Spark sang Python

---

## Support

Náº¿u gáº·p váº¥n Ä‘á»:
1. Check console logs
2. Verify MongoDB connection
3. Test vá»›i Python mode trÆ°á»›c
4. Contact dev team vá»›i error details
